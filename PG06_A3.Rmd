---
title: "PG06_A3"
author:
  - Lachlan Moody ID27809951
  - Abhishek Sinha ID31322743
  - Sen Wang ID30382106
  - Yiwen Zhang ID31203019
date: "04/11/2020"
output: html_document
---
```{r setup}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE,   warning = FALSE, message = FALSE, 
                      error = FALSE, tidy.opts = list(width.cutoff=60), tidy=TRUE, fig.align = 'center')
options(digits = 3)
```

```{r libraries}
library(tidyverse)
library(bayess)
library(broom)
library(car)
library(GGally)
library(meifly)
library(patchwork)
library(kableExtra)
library(boot)
```

```{r data}
data(caterpillar)
cat <- as_tibble(caterpillar)
data_desc <- tibble("variable" = c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "y"),
       "description" = c("altitude (in meters)", "slope (in degrees)", "number of pine trees in the area", "height (in meters) of the tree sampled at the center of the area", "orientation of the area (from 1 if southbound to 2 otherwise)", "height (in meters) of the dominant tree", "number of vegetation strata", "mix settlement index (from 1 if not mixed to 2 if mixed)", "logarithmic transform of the average number of nests of caterpillars per tree"))
```

## Part B: Multiple Linear Regression

```{r datadesc}
data_desc %>%
  kable(caption = "Description of variables in `caterpillar` dataset") %>%
  kable_styling(bootstrap_options = c("striped", "border"))
```

```{r scatmat, fig.height=10, fig.width=10}
ggscatmat(cat, columns=c(1:9)) + 
  theme(geom.text.size = 7, strip.text.x = element_text(size = 12)) +
  theme_bw(base_size = 7)
```


### Q10. [10 marks]  

The *ggscatmat()* function used above comes from the **GGally** package and is used to generate a scatterplot matrix for the caterpillar data set. This data is based on a 1973 study of pine processionary caterpillars and contains a response variable y which relates to the log transform of the number of nests per unit, and 8 potential explanatory variables.

This matrix is divided into three distinct parts and from this we can gather information about the complete set of regressors (x1 through x8) and the response variable y.  

A description of the data from the helpfile and also been included in the table above to provide context for these observations.   

**i)** The lower triangle of the table provides a scatter plot for each variable plotted against all others. The variables in the top margin are plotted on the x-axis and those on the left are plotted on the y-axis. This can used to quickly visualise the relationship between any two variables in the data set and see if any are suitable for further analysis.   

Looking at this for the caterpillar data, while there doesn't appear to be much association for the first two variables, x3 shows somewhat of an a relationship with x6 and a stronger one with x7. This makes sense considering considering x3 relates to the number of pine trees in the area being analysed. As this increases it is probable that both the height of the dominant tree in the area (x6) also increases as there is a larger sample, also the amount of vegetation strata (x7) would also increase as a higher number of trees may suggest an area is more fertile and suitable for growing more plants.  

Moving to the right, x4 (height of the centre tree in the area) has a similarly strong relationship with x6 (height of the tallest tree in the area). Again, this makes sense to be highly associated as areas that can produce taller trees are also more likely to be able to produce tall trees in general, thus increasing the chance that the tree in the centre would be taller.  

Next across, x6 (height of the tallest tree) and x7 (number of vegetation strata) also display a high degree of association which is reasonable given that an area that produces overly tall trees should have high quality soil that increases the amount of vegetation strata in the area.  

Finally, along the bottom row, we see what appears to be a negative association for all the x variables with y along the logarithmic scale that it has been transformed by.  


**ii)** The top right triangle in this matrix display the Pearson correlation coefficient for the relationships between different variables. First examining just the different x variables, almost all are correlated positively, indicating that as one increases so does the others. The only ones that do not are x5 with x2, x8 with x1, and x8 with x4. However, these values are all small and do not indicate any strong linear relationship.  

Looking at the realtionships discussed previously, the correlation for x3 (number of pine trees) with x6 (height of the tallest tree) and x7 (number of vegetation strata) are 0.76 and 0.88 respectively. This supports that these two values are highly positively correlated. Similar results are seen for x4 (height of the centre tree in the area) and x6 (height of the tallest tree in the area) with a correlation of 0.77 and also for x6 (height of the tallest tree) and x7 (number of vegetation strata) with a correlation of 0.85. This provides evidence that there may be some collinearity between the variables discussed.  

Interestingly, all x variables share a negative correlation with the y response variable, with x7(number of vegetation strata) being the strongest. With y relating to the number of caterpillar nest per tree it makes sense to be negatively correlated with x3 (the number of trees) as this gives the caterpillars area to spread out, which is supported by it having the second strongest correlation at -0.56. At discussed previously, this variable is strongly positively correlated with x6 and x7 making it reasonable for them to also have strong negative correlations with y. Also it may be likely that a high altitude (measured by x1) and a steep slope (measured by x2), are not ideal caterpillar habitats and may cause the somewhat strong negative correlation observed.  


**iii)** Finally, the main diagonal displays denisty plots for each variable. Variables x1, x2, and x6 are all approximately normal with varying amounts of positive skew while x4 and x5 are also normal they are instead negatively skewed. Meanwhile x7 and x8 display multi-modality which is reasonable considering there is a fixed number of vegetable strata, and the mix settlement index has a binary outcome of only 1 or 2. Meanwhile examining the response variable, y, it appears that many of the observed values were quite low with quite a long tail to the right. This suggests that most habitats support a small number of caterpillar nests.


### Q11. [5 marks] 
```{r q11}
modelf <- lm(data = cat, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8)
car::vif(modelf) 

```


### Q12. [10 marks] 
The VIF, or variable inflation factor, measures the degree of collinearity between the explanatory variables within a model. This is similar to the visual inference conducted above in the scatter plot matrix but instead provides an automated way to make this comparison. This involves calculating the r-squared value for each variable on all other variables. In general, values for VIF greater than 10 are considered to high.  

This can be used to identify redundant variables where the information is already provided by other variables in the model. It is desirable to identify and remove such regressors as if there is multicollinearity, there tends to be high standard errors on the coefficients for that regressor - that is the weight applied to each of the associated regressors becomes arbitrary.  

Looking at the output for modelf above, variable x6 is the only one with a VIF above the cuttoff value of 10. This variable relates to the height (in meters) of the dominant tree, and as discussed previously, is highly positively correlated with x3, x4, and x7 - or the number of pine trees, height of the centre tree in the area, and number of vegetation strata respectively. As mentioned these associations are reasonable given that more trees (x3) would increase the sample from which the highest tree can be drawn from, a higher center tree would indicate there are taller trees in the area, and an increased about of vegetation would indicate an environment condusive to growing large trees. As such the predictive power of x6 is already captured by these variables and the rest of the data. That means, according to the VIF, this variable should be removed from consideration. This is also desirable as it reduced the total number of regressors for the model without losing its predictive power, leading to the best simplest model.  

```{r ensemble}
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 

all_mod <- quiet(fitall(y=cat$y,x=cat[,-c(6,9)], method="lm"))
summary(all_mod)
```


### Q13. [5 marks] 
```{r q13}
nrow(summary(all_mod))
nmod <- nrow(summary(all_mod))
```

The *all_mod* object contains an ensemble of 127 models. This number was caluclated by counting the number of rows within the data summary. This can also be confirmed by examing the model column in the data summary which has the last model as 127.

```{r modperform, fig.height=4, fig.width=6}
all_mod_s <- all_mod %>%
  map_df(glance) %>%
  mutate(model = nmod) %>%
  mutate(negBIC = -1*BIC, negAIC = -1*AIC) 

label <- NULL
for (i in nmod) {
  l <- as.character(summary(all_mod[[i]])$call)[2]
  label <- c(label,
    substr(l, 5, str_length(l)))
}

all_mod_s_long <- all_mod_s %>%
  gather(fit_stat, val, adj.r.squared, negAIC, 
         negBIC, logLik, r.squared) %>%
  group_by(fit_stat, df) %>% 
  mutate(rank = min_rank(desc(val)))

p1 <- ggplot(all_mod_s_long, aes(df, val)) + 
  geom_point() + 
  geom_line(data=filter(all_mod_s_long, rank == 1)) + 
  facet_wrap(~fit_stat, ncol = 5, scales = "free_y") + 
  xlab("Number of regressors (exclduing the intercept)") + 
  ylab("Values") + 
  theme_bw(base_size = 10)

p1 + 
  ggtitle("Ensemble performance")
```



```{r bestperform}
print("Adjusted R-squared")
indexadjRsq<-c(1:nmod)[all_mod_s$adj.r.squared==max(all_mod_s$adj.r.squared)]
indexadjRsq
max_adjRsq <- all_mod[[indexadjRsq]]
max_adjRsq

print("log-Likelihood")
indexlogLik<-c(1:nmod)[all_mod_s$logLik==max(all_mod_s$logLik)]
indexlogLik
max_logLik <- all_mod[[indexlogLik]]
max_logLik

print("Negative AIC")
indexAIC<-c(1:nmod)[all_mod_s$negAIC==max(all_mod_s$negAIC)]
indexAIC
max_AIC <- all_mod[[indexAIC]]
max_AIC

print("Negative BIC")
indexBIC<-c(1:nmod)[all_mod_s$negBIC==max(all_mod_s$negBIC)]
indexBIC
max_BIC <- all_mod[[indexBIC]]
max_BIC

print("R-squared")
indexRsq<-c(1:nmod)[all_mod_s$r.squared==max(all_mod_s$r.squared)]
indexRsq
max_Rsq <- all_mod[[indexRsq]]
max_Rsq
```

### Q14. [20 marks] 
Based on the ensemble of models produced, five potentially "best" models are selected based on those with the highest adjusted r-squared, log-likelihood, negative AIC, negative BIC and r-squared values. However, as r-squared and log-likelihood will never decrease when new regressors are added they should not be used to compare models with different numbers of regressors as in this scenario. The other three measures, adjusted r-squared, negative AIC and negative BIC introduce penalty terms for the number of regressors thus making them for useful for comparison in this scenario.  

Based on this, there are only two models for consideration. Model 55 as the one with highest adjusted r-squared and negative AIC and model 35 as the one with the highest BIC. The formula foe each of these models has been saved as `mod55` and `mod35` below. Additionally, a summary of each models performance has been output. Note that the table shows the AIC and BIC values rather than their negatives. So while model 35's BIC is lower, between these two it is better considering the aim is to maximise the negative value.  

```{r q14i}
mod55 <- lm(formula = y ~ x1 + x2 + x3 + x5 + x7, data = cat)
mod35 <- lm(formula = y ~ x1 + x2 + x7, data = cat)

bind_rows(glance(mod55),
          glance(mod35), .id = "model") %>%
  mutate(model = c("model 55", "model 35")) %>%
  kable(caption = "Model summary statistics") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))
```  
Firstly regarding the number of regressors in each model, looking back to the table titled *'Ensemble Performance'*, we can see that both adjusted r-squared and negative AIC both peak at 5 regressors each while negative BIV peaks at 3. This matches the form of model 55 and 35 respectively. The difference in model preference for the negative AIC and BIC values is not unexpected as BIC penalises model complexity more heavily than AIC or adjusted r-squared. Thus it produces a higher value for model 35 which only has three regressors plus an intercept over model 55 which has 5 regressors plus an intercept.  

So as both models have some level of validity the next step is to examine the residual values of each model. This can be visualised in three ways - plotting a histogram of the residuals, producing a normal probability (or quantile-quantile) plot, and ploting a residuals versus fitted plot. This can be seen in the output below.

```{r q14ii, fig.height=6, fig.width=8}
model_hist <- function(model, title){
  augment(model) %>%
    ggplot(aes(x= .resid, y = ..density..)) +
    geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
    theme_bw() +
    ggtitle(paste("Histrogram of residuals for", title))
}

model_qq <- function(model, title){
    augment(model) %>%
      ggplot(aes(sample = .resid)) +
      geom_qq(color = "#e15759",
              alpha = 0.5) +
      geom_qq_line(color = "#4e79a7",
                   size = 1) +
      theme_bw() +
      ggtitle(paste("Q-Q plot for", title))
}

model_resid <- function(model, title){
    augment(model) %>%
      ggplot(aes(x = .fitted, y = .resid)) +
      geom_point(color = "#e15759",
              alpha = 0.5) +
      geom_hline(yintercept = 0, 
                 color = "#4e79a7",
                 size = 1) +
      theme_bw() +
      ggtitle(paste("Residual plot for", title))
}

p1 <- model_hist(mod55, "model 55")
p2 <- model_qq(mod55, "model 55")
p3 <- model_resid(mod55, "model 55")

p4 <- model_hist(mod35, "model 35")
p5 <- model_qq(mod35, "model 35")
p6 <- model_resid(mod35, "model 35")

(p1 | p4) /
(p2 | p5) /
(p3 | p6)

```

Starting from the top row, the residuals for model 55 appear to be more normally distributed than model 35 as it appears to have some slight positive skew in its distribution. This indicates that the normality assumption is more likely to be true for model 55 as the variance is normally distributed.  

In the second row, the normal probability plot for each model is shown. This plots the quantiles sampled from the data against the standard normal distribution. If the points fall along the line, this indicates that the residuals come from a normal distribution. As both models fit this line quite well they both pass this check.  

The third plot shows the residuals of each model against its fitted value. The plot is used to detect non-linearity, unequal error variances, and outliers which can be detected by the appearance of any type of pattern. For model 55 there appears to be no visible pattern in the data which strengthens the evidence for this models use. In comparison for model 35 there appears to be somewhat of an upward trend towards the end of the plot indicating that there still may be some relationship not captured by the model.  

An additional method that can be used is to plot the residuals against all available regressors in the data, even those not included in either model. This plot is displayed below. A well-fitted model should likewise not show any obvious pattern. Comparing the models across rows, most of the plots look similar with random variation around 0 except for x2 under model 35. Towards the end of the data there is a clearly identifiable curve indicating that there is an underlying relationship that mass been missed. This provides more strength of evidence for model 55 being the preferred model.  

```{r 14iii, fig.height=10, fig.width=10}
residregress <- function(model, title){
cat %>% left_join(augment(model)) %>%
  pivot_longer(x1:x8) %>%
    ggplot(aes(x = value, y = .resid)) +
    geom_point(color = "#e15759",
            alpha = 0.5) +
    facet_wrap(~name, scales ="free", ncol = 1) +
    ggtitle(paste("Residuals against available regressors for", title)) +
    geom_hline(yintercept = 0, 
                 color = "#4e79a7",
                 size = 1) +
    theme_bw()
}

residregress(mod55, "model 55") + residregress(mod35, "model 35")
```
As a final check, the Leverage and Cook's Distance values for each model can be compared relative to their cuttoff value which is calculated as 2p/n where p is the number of regressors in the model and n in the number of observations. Rather than being a strict cuttoff value like VIF, here it is only values far away from the threshold that may be of concern.  

These values deal with individual obsevations in different ways. Leverage is a measure of how distant an observed independent variable is from other observations of the same variable. Values with a high leverage has a greater influence on the fitted regression line. For model 55 there appears to be two observations with noticeably high leverage compared to one for model 35.  

We can examine this further using Cook's Distance which measures the effect of deleting a given observation using a combination of each observations leverage and residual values. Here neither model displays any obersvations that require further analysis. As neither have values that fall outside this threshold it would appear both models don't have any overly influential observations.  

```{r q14iv}
n = nrow(cat)

p55 = nrow(tidy(mod55))
t55 = 2*p55/n

p35 = nrow(tidy(mod35))
t35 = 2*p35/n

leverage_plot <- function(model, t, title){
  augment(model) %>%
  ggplot(aes(x = .hat)) +
  geom_bar(colour = "#4e79a7", fill = "#4e79a7") +
  geom_vline(xintercept = t, colour = "#e15759") + 
  theme_bw() +
  ggtitle(paste("Leverage values for", title))
}

cooks_plot <- function(model, t, title){
  augment(model) %>%
    ggplot(aes(x = .cooksd)) +
    geom_bar(colour = "#4e79a7", fill = "blue") +
    geom_vline(xintercept = t, colour = "#e15759") +
    theme_bw() +
    ggtitle(paste("Cook's Distance values for", title))
}

p7 <- leverage_plot(mod55, t55, "model 55")
p8 <- cooks_plot(mod55, t55, "model 55")

p9 <- leverage_plot(mod35, t35, "model 35")
p10 <- cooks_plot(mod35, t35, "model 35")


(p7 | p9) /
(p8 | p10)


```

Based on the output and discussion conducted above, model 55 has been chosen as the preferred model as it recorded higher values in regards to both adjusted r-squared and negative AIC, it's residual plots displayed less evidence of breaking the model assumptions, and it had no influential observations when using Cook's Distance and only one more than model 35 when using leverage.  

The form of model 55 has been saved in the output below as modelp.  

```{r q14v}
modelp <- lm(formula = y ~ x1 + x2 + x3 + x5 + x7, data = cat)
```


### Q15. [5 marks] 
```{r q15i}
modelp
```
Based on the above analysis, the preferred model is model 55 which is saved as modelp. The form of this model can be seen in the above output and can be written as:
$$ y = 8.167 - 0.0026 \times x1 - 0.0361 \times x2 + 0.0396 \times x3 -0.692 \times x5 - 1.108 \times x7 $$ 

Or with abbreviated variable names as:
$$ y = 8.167 - 0.0026 \times altitude - 0.0361 \times slope + 0.0396 \times trees -0.692 \times orientation - 1.108 \times vegetation $$

CLT-based confidence levels can also be constructed for the regression coefficients using the *confit* function and are provided in the table below:

```{r 15ii}
clt <- confint(modelp) %>%
  as_tibble() %>%
  mutate(coefficient = c("Incercept", "x1", "x2", "x3", "x5", "x7")) %>%
  dplyr::select(coefficient, `2.5 %`, `97.5 %`)

clt %>% kable(caption = "CLT-based confidence intervals") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))
```
An additional statistic of interest is the estimated standard deviation of the residuals or, also named, the residual standard deviation. This can be extracted using the *sigma* function is shown in the output below to be .531. This value tells us the standard deviation of the residual values which is the difference between the obseved and model fitted values.

```{r 15iii}
sigma(modelp)
```

### Q16. [10 marks] 

```{r boot}
modelp <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = cat) 
tidyp <- tidy(modelp)

R <- 1000
n <- nrow(cat)

R_coeffs <- tibble(b0 = rep(0,R), b1 = rep(0, R), b2 = rep(0, R), b3=rep(0,R), b4=rep(0,R), b5 = rep(0, R))

set.seed(2020) 
for(j in (1:R)){
  temp <- cat %>% slice_sample(n=n, replace=TRUE)
  tempf <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = temp)
  tidyf <- tidy(tempf)
  R_coeffs[j,] <- t(tidyf$estimate)
}

beta_coeff <- function(b){
  R_coeffs %>%
  pull({{b}}) %>%
  quantile(c(0.025, 0.975))
}

boot_interval <- bind_rows(
  beta_coeff(b0),
  beta_coeff(b1),
  beta_coeff(b2),
  beta_coeff(b3),
  beta_coeff(b4),
  beta_coeff(b5))



boot_interval <- boot_interval %>%
  mutate(coefficient = c("b0", "b1", "b2", "b3", "b4", "b5")) %>%
  dplyr::select(coefficient, `2.5%`, `97.5%`)

boot_interval %>%
  kable(caption = "Bootstrap-based confidence intervals") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))

modelp
  
```
The code above has been used to generate 1000 values from the bootstrap sampling distribution of the regression coefficients from the preferred model, modelp. This has been sampled from the original caterpillar data with replacement. The model of form modelp has then been refit for each sample and the regression coefficients saved for each of the 1000 replications. From this bootstrap sample, a 95% confidence interval was produced for each of the regression coefficients within this model and is shown in the table outputted above.  

This interval helps provide an insight into the variability present in each estimate and, although not formally, helps identify which are statistically different from 0 and are therefore having an effect on the response variable - the logarithmic transform of the average number of nests of caterpillars per tree. The lower 2.5% and 97.5% quantiles of the data shown are the end points of this interval and represent the degree of uncertainty surrounding these estimated coefficients (which are shown in the model output above) in relation to the 'true' population values. For example, the intercept term b0 has an observed value of 8.167, however this only comes from a sample from the entire population. Looking at the produced confidence interval this can be expanded to say that we are 95% confident the 'true' value of b0 falls between 5.106 and 11.673 using the bootstrap method. The same method can be applied to all other estimates shown.  

This can also be visualised by producing a histogram of the bootstrap distribution for each regression coefficients, which is shown in the output below. On this plot, the observed value of eat beta is shown by a black line whearas the two grey lines correspond to the lower and upper bounds of the confidence interval. This shows what was discussed above where the grey bars capture 95% of the simulated values for each estimate.

```{r bootplot, fig.height=6, fig.width=8}
boot_plot <- R_coeffs %>%
  pivot_longer(names_to = "coefficient", cols = b0:b5) %>%
  left_join(boot_interval) %>%
  left_join(tidyp %>% mutate(coefficient = c("b0", "b1", "b2", "b3", "b4", "b5")) %>%
  dplyr::select(coefficient, estimate))

boot_plot %>% ggplot(aes(x = value, y = ..density..)) +
  geom_density(fill = "#e15759", colour = "#4e79a7") +
  geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
  geom_vline(aes(xintercept = `2.5%`), size = 1, colour = "dark grey") +
  geom_vline(aes(xintercept = `97.5%`), size = 1, colour = "dark grey") +
  geom_vline(aes(xintercept = estimate), size = 1) +
  facet_wrap(~coefficient, scales = "free") +
  theme_bw() +
  ggtitle("Bootstrap sampling confidence intervals for regression coefficients") +
  xlab("coefficient estimate")


```


## Part C: Additional Questions for ETC5242 Groups


### Q17. [20 marks] 
The code displayed below is used to conduct a two sided permutation test for b1 for the preferred model, designated as modelp. This test can be used to conduct the following hypothesis test at a 5% significance level.

$$ H_{0}: \beta_{1} = 0 $$
$$ H_{1}: \beta_{1} \neq 0 $$

Similar to the bootstrap process described earlier, here the data is sampled from the empirical data but this time without replacement. Additionally, as we are only testing the value of $\beta_{1} $ we only shuffle the values of the associated explanatory variable, in this case x1. Once this is done we re-fit modelp to this new sample and extract the estimate of the coefficient for $ \beta_{1} $. As with the bootstrap sample, we repeat this process 1,000 times.  

This process is done to break any existing association between the regressor and the response variable, which is y, in our collected data. The resulting distribution from these permutations represents the hypothetical distribution of $\beta_{1} $ under $ H{0} $. In regards to this model, this would mean that x1 (altitude in m) is not having a statistically significant effect on the number of caterpillar nests per tree (y variable). The estimate calculated for our model from the observed data can then be compared to this null distribution in order to calculate the p-value for the permutation test.

```{r q17i}
set.seed(2020)

cat2 <- cat 
n <- nrow(cat2) 
R <- 1000 


Rcat2 <- cat2

for (r in 1:R) {
  Rcat2 <- Rcat2 %>% mutate(x1 = sample(cat2$x1, n, replace = FALSE))
  tempf <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = Rcat2)
  tidyf <- tidy(tempf)
  R_coeffs[j,] <- t(tidyf$estimate)
}
```

The p-value calculated from this will be used to determine the outcome of the hypothesis tests shown earlier. As we are using a 5% confidence level we requiire a p-value of 0.05 or smaller for their to be enough evidence to reject the  null hypothesis in favour of the alternative. To calculate this p-value, the proportion of randomised samples that are as or more extreme than the observed value are required which is designated as `b1obs` and has a value of -0.0026. We use this value to filter the permuted samples with estimates more extreme than this value. As this is a two-sided test we use the absolute value of both as we are interested in any difference regardless of direction.  


The code below performs this calculation and is saved as `pval`, and calling this value we see that it is 0.52. This means that 52% of our data sets sampled from the null distribution produced a $ \beta_{1} $ more extreme than that observed from the 'real' data. Since this value is above 0.05 we cannot reject $ H_{0}$ as there is not enough strength of evidence to do so. Thus it appears the value of $ \beta_{1} $ is not significantly different from 0. 

```{r q17ii}
b1obs <- tidyp %>% filter(term == "x1") %>%
  pull(estimate)

pval <- R_coeffs %>%
  filter(abs(b1) >= abs(b1obs)) %>%
  nrow()/R

pval
```
This can be viusalised, as with the bootstrap, by producing a histogram of the permuted samples. Here the black line relates to the observed value of $ \beta_{1}$ from the modelp. From this we can see that roughly half of the values produced were more extreme than what was modeled. This is further evidence for the failure to reject $ H_{0}$.

```{r q17iii}
R_coeffs %>%
  dplyr::select(b1) %>%
  mutate(b1obs = b1obs) %>%
  ggplot(aes(x = b1)) +
  geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
  geom_vline(xintercept = b1obs, size = 1) +
  theme_bw() +
  ggtitle("Permutation test for b1 from modelp") +
  xlab("permuted value of b1")
```


### Q18. [15 marks] 
An additional method for model validation is leave one out cross validation, denoted as LOOCV. This method works by leaving out one data point and building the model on the remaining data. The produced model is then used to predict the value of the removed observation. This process is repeated for all of the observations. Once complete the overall prediction error can be calculated by taking the average of the test errors associated with each of the predictions. The function created below calculates this value for each model by taking the mean of the residuals over the leverage squared. The output is provided in the table.

```{r q18}
loocv <- function(model){
  h = lm.influence(model)$h
  mean((residuals(model)/(1-h))^2)
}

tibble("model" = c("modelf", "modelp")) %>%
  mutate("loocv" = c(loocv(modelf), loocv(modelp))) %>%
  kable(caption = "LOOCV values for modelf and modelp") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))
```
From this table we see that modelp has a lower LOOCV value and therefore a lower overall error - even with fewer regressors. This means that the preferred model performs better than the full model at predicting the response variable y -  the logarithmic transform of the average number of nests of caterpillars per tree. This then supports the process undertaken to determine the best model, and the removal of x4, x6, and x8 from the linear model. Respectively, this means that it is unlikely that the height of the center tree, the height of the dominant tree and the settlement index are associated with the number of caterpillar nests per tree.
