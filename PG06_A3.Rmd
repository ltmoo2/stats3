---
title: "PG06_A3"
author:
  - Lachlan Moody ID27809951
  - Abhishek Sinha ID31322743
  - Sen Wang ID30382106
  - Yiwen Zhang ID31203019
date: "04/11/2020"
output: html_document
---
```{r setup}
knitr::opts_chunk$set(echo = TRUE, eval=TRUE,   warning = FALSE, message = FALSE, 
                      error = FALSE, tidy.opts = list(width.cutoff=60), tidy=TRUE, fig.align = 'center')
options(digits = 3)
```

```{r libraries}
library(tidyverse)
library(bayess)
library(broom)
library(car)
library(GGally)
library(meifly)
library(patchwork)
library(kableExtra)
library(boot)
```

```{r data}
data(caterpillar)
cat <- as_tibble(caterpillar)
data_desc <- tibble("variable" = c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "y"),
       "description" = c("altitude (in meters)", "slope (in degrees)", "number of pine trees in the area", "height (in meters) of the tree sampled at the center of the area", "orientation of the area (from 1 if southbound to 2 otherwise)", "height (in meters) of the dominant tree", "number of vegetation strata", "mix settlement index (from 1 if not mixed to 2 if mixed)", "logarithmic transform of the average number of nests of caterpillars per tree"))
```

## Part B: Multiple Linear Regression

```{r datadesc}
data_desc %>%
  kable(caption = "Description of variables in `caterpillar` dataset") %>%
  kable_styling(bootstrap_options = c("striped", "border"))
```

```{r scatmat, fig.height=10, fig.width=10}
ggscatmat(cat, columns=c(1:9)) + 
  theme(geom.text.size = 7, strip.text.x = element_text(size = 12)) +
  theme_bw(base_size = 7)
```


### Q10. [10 marks]  

The *ggscatmat()* function used above comes from the **GGally** package and is used to generate a scatterplot matrix for the caterpillar data set. This data is based on a 1973 study of pine processionary caterpillars and contains a response variable y which relates to the log transform of the number of nests per unit, and 8 potential explanatory variables.

This matrix is divided into three distinct parts and from this we can gather information about the complete set of regressors (x1 through x8) and the response variable y.  

A description of the data from the helpfile and also been included in the table above to provide context for these observations.   

**i)** The lower triangle of the table provides a scatter plot for each variable plotted against all others. The variables in the top margin are plotted on the x-axis and those on the left are plotted on the y-axis. This can used to quickly visualise the relationship between any two variables in the data set and see if any are suitable for further analysis.   

Looking at this for the caterpillar data, while there doesn't appear to be much association for the first two variables, x3 shows somewhat of an a relationship with x6 and a stronger one with x7. This makes sense considering considering x3 relates to the number of pine trees in the area being analysed. As this increases it is probable that both the height of the dominant tree in the area (x6) also increases as there is a larger sample, also the amount of vegetation strata (x7) would also increase as a higher number of trees may suggest an area is more fertile and suitable for growing more plants.  

Moving to the right, x4 (height of the centre tree in the area) has a similarly strong relationship with x6 (height of the tallest tree in the area). Again, this makes sense to be highly associated as areas that can produce taller trees are also more likely to be able to produce tall trees in general, thus increasing the chance that the tree in the centre would be taller.  

Next across, x6 (height of the tallest tree) and x7 (number of vegetation strata) also display a high degree of association which is reasonable given that an area that produces overly tall trees should have high quality soil that increases the amount of vegetation strata in the area.  

Finally, along the bottom row, we see what appears to be a negative association for all the x variables with y along the logarithmic scale that it has been transformed by.  


**ii)** The top right triangle in this matrix display the Pearson correlation coefficient for the relationships between different variables. First examining just the different x variables, almost all are correlated positively, indicating that as one increases so does the others. The only ones that do not are x5 with x2, x8 with x1, and x8 with x4. However, these values are all small and do not indicate any strong linear relationship.  

Looking at the realtionships discussed previously, the correlation for x3 (number of pine trees) with x6 (height of the tallest tree) and x7 (number of vegetation strata) are 0.76 and 0.88 respectively. This supports that these two values are highly positively correlated. Similar results are seen for x4 (height of the centre tree in the area) and x6 (height of the tallest tree in the area) with a correlation of 0.77 and also for x6 (height of the tallest tree) and x7 (number of vegetation strata) with a correlation of 0.85. This provides evidence that there may be some collinearity between the variables discussed.  

Interestingly, all x variables share a negative correlation with the y response variable, with x7(number of vegetation strata) being the strongest. With y relating to the number of caterpillar nest per tree it makes sense to be negatively correlated with x3 (the number of trees) as this gives the caterpillars area to spread out, which is supported by it having the second strongest correlation at -0.56. At discussed previously, this variable is strongly positively correlated with x6 and x7 making it reasonable for them to also have strong negative correlations with y. Also it may be likely that a high altitude (measured by x1) and a steep slope (measured by x2), are not ideal caterpillar habitats and may cause the somewhat strong negative correlation observed.  


**iii)** Finally, the main diagonal displays denisty plots for each variable. Variables x1, x2, and x6 are all approximately normal with varying amounts of positive skew while x4 and x5 are also normal they are instead negatively skewed. Meanwhile x7 and x8 display multi-modality which is reasonable considering there is a fixed number of vegetable strata, and the mix settlement index has a binary outcome of only 1 or 2. Meanwhile examining the response variable, y, it appears that many of the observed values were quite low with quite a long tail to the right. This suggests that most habitats support a small number of caterpillar nests.


### Q11. [5 marks] 
```{r q11}
modelf <- lm(data = cat, formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8)
car::vif(modelf) 

```


### Q12. [10 marks] 
The VIF, or variable inflation factor, measures the degree of collinearity between the explanatory variables within a model. This is similar to the visual inference conducted above in the scatter plot matrix but instead provides an automated way to make this comparison. This involves calculating the r-squared value for each variable on all other variables. In general, values for VIF greater than 10 are considered to high.  

This can be used to identify redundant variables where the information is already provided by other variables in the model. It is desirable to identify and remove such regressors as if there is multicollinearity, there tends to be high standard errors on the coefficients for that regressor - that is the weight applied to each of the associated regressors becomes arbitrary.  

Looking at the output for modelf above, variable x6 is the only one with a VIF above the cuttoff value of 10. This variable relates to the height (in meters) of the dominant tree, and as discussed previously, is highly positively correlated with x3, x4, and x7 - or the number of pine trees, height of the centre tree in the area, and number of vegetation strata respectively. As mentioned these associations are reasonable given that more trees (x3) would increase the sample from which the highest tree can be drawn from, a higher center tree would indicate there are taller trees in the area, and an increased about of vegetation would indicate an environment condusive to growing large trees. As such the predictive power of x6 is already captured by these variables and the rest of the data. That means, according to the VIF, this variable should be removed from consideration. This is also desirable as it reduced the total number of regressors for the model without losing its predictive power, leading to the best simplest model.  

```{r ensemble}
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 

all_mod <- quiet(fitall(y=cat$y,x=cat[,-c(6,9)], method="lm"))
summary(all_mod)
```


### Q13. [5 marks] 
```{r q13}
nrow(summary(all_mod))
nmod <- nrow(summary(all_mod))
```

The *all_mod* object contains an ensemble of 127 models. This number was caluclated by counting the number of rows within the data summary. This can also be confirmed by examing the model column in the data summary which has the last model as 127.

```{r modperform, fig.height=4, fig.width=6}
all_mod_s <- all_mod %>%
  map_df(glance) %>%
  mutate(model = nmod) %>%
  mutate(negBIC = -1*BIC, negAIC = -1*AIC) 

label <- NULL
for (i in nmod) {
  l <- as.character(summary(all_mod[[i]])$call)[2]
  label <- c(label,
    substr(l, 5, str_length(l)))
}

all_mod_s_long <- all_mod_s %>%
  gather(fit_stat, val, adj.r.squared, negAIC, 
         negBIC, logLik, r.squared) %>%
  group_by(fit_stat, df) %>% 
  mutate(rank = min_rank(desc(val)))

p1 <- ggplot(all_mod_s_long, aes(df, val)) + 
  geom_point() + 
  geom_line(data=filter(all_mod_s_long, rank == 1)) + 
  facet_wrap(~fit_stat, ncol = 5, scales = "free_y") + 
  xlab("Number of regressors (exclduing the intercept)") + 
  ylab("Values") + 
  theme_bw(base_size = 10)

p1
```



```{r bestperform}
print("Adjusted R-squared")
indexadjRsq<-c(1:nmod)[all_mod_s$adj.r.squared==max(all_mod_s$adj.r.squared)]
indexadjRsq
max_adjRsq <- all_mod[[indexadjRsq]]
max_adjRsq

print("log-Likelihood")
indexlogLik<-c(1:nmod)[all_mod_s$logLik==max(all_mod_s$logLik)]
indexlogLik
max_logLik <- all_mod[[indexlogLik]]
max_logLik

print("Negative AIC")
indexAIC<-c(1:nmod)[all_mod_s$negAIC==max(all_mod_s$negAIC)]
indexAIC
max_AIC <- all_mod[[indexAIC]]
max_AIC

print("Negative BIC")
indexBIC<-c(1:nmod)[all_mod_s$negBIC==max(all_mod_s$negBIC)]
indexBIC
max_BIC <- all_mod[[indexBIC]]
max_BIC

print("R-squared")
indexRsq<-c(1:nmod)[all_mod_s$r.squared==max(all_mod_s$r.squared)]
indexRsq
max_Rsq <- all_mod[[indexRsq]]
max_Rsq
```

### Q14. [20 marks] 
```{r q14i}
mod55 <- lm(formula = y ~ x1 + x2 + x3 + x5 + x7, data = cat)
mod35 <- lm(formula = y ~ x1 + x2 + x7, data = cat)

bind_rows(glance(mod55),
          glance(mod35), .id = "model") %>%
  mutate(model = c("model 55", "model 35")) %>%
  kable(caption = "Model summary statistics") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))
```  


```{r q14ii, fig.height=6, fig.width=8}
model_hist <- function(model, title){
  augment(model) %>%
  ggplot(aes(x= .resid)) +
  geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
  theme_bw() +
    ggtitle(paste("Histrogram of residuals for", title))
}

model_qq <- function(model, title){
    augment(model) %>%
    ggplot(aes(sample = .resid)) +
    geom_qq(color = "#e15759",
            alpha = 0.5) +
    geom_qq_line(color = "#4e79a7",
                 size = 1) +
    theme_bw() +
    ggtitle(paste("Q-Q plot for", title))
}

model_resid <- function(model, title){
    augment(model) %>%
    ggplot(aes(x = .fitted, y = .resid)) +
    geom_point(color = "#e15759",
            alpha = 0.5) +
    geom_hline(yintercept = 0, 
               color = "#4e79a7",
               size = 1) +
    theme_bw() +
    ggtitle(paste("Residual plot for", title))
}

p1 <- model_hist(mod55, "model 55")
p2 <- model_qq(mod55, "model 55")
p3 <- model_resid(mod55, "model 55")

p4 <- model_hist(mod35, "model 35")
p5 <- model_qq(mod35, "model 35")
p6 <- model_resid(mod35, "model 35")

(p1 | p4) /
(p2 | p5) /
(p3 | p6)

```

```{r 14iii, fig.height=10, fig.width=10}
residregress <- function(model, title){
cat %>% left_join(augment(model)) %>%
  pivot_longer(x1:x8) %>%
    ggplot(aes(x = value, y = .resid)) +
    geom_point(color = "#e15759",
            alpha = 0.5) +
    facet_wrap(~name, scales ="free", ncol = 1) +
    ggtitle(paste("Residuals against available regressors for", title)) +
    theme_bw()
}

residregress(mod55, "model 55") + residregress(mod35, "model 35")
```



```{r q14iv}
n = nrow(cat)

p55 = nrow(tidy(mod55))
t55 = 2*p55/n

p35 = nrow(tidy(mod35))
t35 = 2*p35/n

leverage_plot <- function(model, t, title){
  augment(model) %>%
  ggplot(aes(x = .hat)) +
  geom_bar(colour = "#4e79a7", fill = "#4e79a7") +
  geom_vline(xintercept = t, colour = "#e15759") + 
  theme_bw() +
  ggtitle(paste("Leverage values for", title))
}

cooks_plot <- function(model, t, title){
  augment(model) %>%
    ggplot(aes(x = .cooksd)) +
    geom_bar(colour = "#4e79a7", fill = "blue") +
    geom_vline(xintercept = t, colour = "#e15759") +
    theme_bw() +
    ggtitle(paste("Cook's Distance values for", title))
}

p7 <- leverage_plot(mod55, t55, "model 55")
p8 <- cooks_plot(mod55, t55, "model 55")

p9 <- leverage_plot(mod35, t35, "model 35")
p10 <- cooks_plot(mod35, t35, "model 35")


(p7 | p9) /
(p8 | p10)


```


```{r q14v}
modelp <- lm(formula = y ~ x1 + x2 + x3 + x5 + x7, data = cat)
```


### Q15. [5 marks] 
```{r q15i}
modelp
```
Based on the above analysis, the preferred model is model 55 which is saved as modelp. The form of this model can be seen in the above output and can be written as:
$ y = 8.167 - 0.0026 \times x1 - 0.0361 \times x2 + 0.0396 \times x3 -0.692 \times x5 - 1.108 \times x7$ 

Or with abbreviated variable names as:
$ y = 8.167 - 0.0026 \times altitude - 0.0361 \times slope + 0.0396 \times trees -0.692 \times orientation - 1.108 \times vegetation$

CLT-based confidence levels can also be constructed for the regression coefficients using the *confit* function and are provided in the table below:

```{r 15ii}
clt <- confint(modelp) %>%
  as_tibble() %>%
  mutate(coefficient = c("Incercept", "x1", "x2", "x3", "x5", "x7")) %>%
  dplyr::select(coefficient, `2.5 %`, `97.5 %`)

clt %>% kable(caption = "CLT-based confidence intervals") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))
```
An additional statistic of interest is the estimated standard deviation of the residuals or, also named, the residual standard deviation. This can be extracted using the *sigma* function is shown in the output below to be .531. This value tells us the standard deviation of the residual values which is the difference between the obseved and model fitted values.

```{r 15iii}
sigma(modelp)
```

### Q16. [10 marks] 

```{r boot}
modelp <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = cat) 
tidyp <- tidy(modelp)

R <- 1000
n <- nrow(cat)

R_coeffs <- tibble(b0 = rep(0,R), b1 = rep(0, R), b2 = rep(0, R), b3=rep(0,R), b4=rep(0,R), b5 = rep(0, R))

set.seed(2020) 
for(j in (1:R)){
  temp <- cat %>% slice_sample(n=n, replace=TRUE)
  tempf <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = temp)
  tidyf <- tidy(tempf)
  R_coeffs[j,] <- t(tidyf$estimate)
}

beta_coeff <- function(b){
  R_coeffs %>%
  pull({{b}}) %>%
  quantile(c(0.025, 0.975))
}

boot_interval <- bind_rows(
  beta_coeff(b0),
  beta_coeff(b1),
  beta_coeff(b2),
  beta_coeff(b3),
  beta_coeff(b4),
  beta_coeff(b5))



boot_interval <- boot_interval %>%
  mutate(coefficient = c("b0", "b1", "b2", "b3", "b4", "b5")) %>%
  dplyr::select(coefficient, `2.5%`, `97.5%`)

boot_interval %>%
  kable(caption = "Bootstrap-based confidence intervals") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"))

modelp
  
```
The code above has been used to generate 1000 values from the bootstrap sampling distribution of the regression coefficients from the preferred model, modelp. This has been sampled from the original caterpillar data with replacement. The model of form modelp has then been refit for each sample and the regression coefficients saved for each of the 1000 replications. From this bootstrap sample, a 95% confidence interval was produced for each of the regression coefficients within this model and is shown in the table outputted above.  

This interval helps provide an insight into the variability present in each estimate and, although not formally, helps identify which are statistically different from 0 and are therefore having an effect on the response variable - the logarithmic transform of the average number of nests of caterpillars per tree. The lower 2.5% and 97.5% quantiles of the data shown are the end points of this interval and represent the degree of uncertainty surrounding these estimated coefficients (which are shown in the model output above) in relation to the 'true' population values. For example, the intercept term b0 has an observed value of 8.167, however this only comes from a sample from the entire population. Looking at the produced confidence interval this can be expanded to say that we are 95% confident the 'true' value of b0 falls between 5.106 and 11.673 using the bootstrap method. The same method can be applied to all other estimates shown.  

This can also be visualised by producing a histogram of the bootstrap distribution for each regression coefficients, which is shown in the output below. On this plot, the observed value of eat beta is shown by a black line whearas the two grey lines correspond to the lower and upper bounds of the confidence interval. This shows what was discussed above where the grey bars capture 95% of the simulated values for each estimate.

```{r bootplot, fig.height=6, fig.width=8}
boot_plot <- R_coeffs %>%
  pivot_longer(names_to = "coefficient", cols = b0:b5) %>%
  left_join(boot_interval) %>%
  left_join(tidyp %>% mutate(coefficient = c("b0", "b1", "b2", "b3", "b4", "b5")) %>%
  dplyr::select(coefficient, estimate))

boot_plot %>% ggplot(aes(x = value, y = ..density..)) +
  geom_density(fill = "#e15759", colour = "#4e79a7") +
  geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
  geom_vline(aes(xintercept = `2.5%`), size = 1, colour = "dark grey") +
  geom_vline(aes(xintercept = `97.5%`), size = 1, colour = "dark grey") +
  geom_vline(aes(xintercept = estimate), size = 1) +
  facet_wrap(~coefficient, scales = "free") +
  theme_bw() +
  ggtitle("Bootstrap sampling confidence intervals for regression coefficients") +
  xlab("coefficient estimate")


```


## Part C: Additional Questions for ETC5242 Groups


### Q17. [20 marks] 
```{r q17}
set.seed(2020)

cat2 <- cat 
n <- nrow(cat2) 
R <- 1000 


Rcat2 <- cat2

for (r in 1:R) {
  Rcat2 <- Rcat2 %>% mutate(x1 = sample(cat2$x1, n, replace = FALSE))
  tempf <- lm(y ~ x1 + x2 + x3 + x5 + x7, data = Rcat2)
  tidyf <- tidy(tempf)
  R_coeffs[j,] <- t(tidyf$estimate)
}

b1obs <- tidyp %>% filter(term == "x1") %>%
  pull(estimate)


pval <- R_coeffs %>%
  filter(abs(b1) >= abs(b1obs)) %>%
  nrow()/R

pval


R_coeffs %>%
  dplyr::select(b1) %>%
  mutate(b1obs = b1obs) %>%
  ggplot(aes(x = b1)) +
  geom_histogram(fill = "#e15759", colour = "#4e79a7", alpha = 0.8) +
  geom_vline(xintercept = b1obs, size = 1) +
  geom_vline(xintercept = -b1obs, size = 1) +
  theme_bw() +
  ggtitle("Permutation test for b1 from modelp") +
  xlab("permuted value of b1")




```


### Q18. [15 marks] 
```{r q18}
loocv <- function(model){
  h = lm.influence(model)$h
  mean((residuals(model)/(1-h))^2)
}


loocv(modelf)
loocv(modelp)
```

